{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading json file!\n",
      "done reading json file!\n",
      "(47757, 11)\n",
      "<class 'numpy.ndarray'>\n",
      "======\n",
      "122.5\n",
      "83.0\n",
      "[[  8.40999980e+01   8.41999970e+01   8.40999980e+01 ...,   4.44828722e-02\n",
      "    0.00000000e+00   1.66666667e+01]\n",
      " [  8.40999980e+01   8.40999980e+01   8.40999980e+01 ...,   3.68835728e-02\n",
      "   -9.99990000e-02   1.66666667e+01]\n",
      " [  8.40999980e+01   8.41999970e+01   8.40999980e+01 ...,   3.05093801e-02\n",
      "   -9.99990000e-02   8.33333333e+00]\n",
      " ..., \n",
      " [  1.07000000e+02   1.07000000e+02   1.07000000e+02 ...,   2.21575909e-02\n",
      "    0.00000000e+00   1.66666667e+01]\n",
      " [  1.07000000e+02   1.07000000e+02   1.07000000e+02 ...,   2.91521094e-02\n",
      "    5.00000000e-01   1.66666667e+01]\n",
      " [  1.07000000e+02   1.07000000e+02   1.07000000e+02 ...,   3.42999275e-02\n",
      "    5.00000000e-01   8.33333333e+00]]\n",
      "(47757, 11)\n"
     ]
    }
   ],
   "source": [
    "## upload date:1110\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# read data.json\n",
    "print(\"start reading json file!\")\n",
    "with open(\"data1.json\") as input_file:\n",
    "    Raw_data = json.load(input_file)\n",
    "print(\"done reading json file!\")\n",
    "\n",
    "### 共47790筆\n",
    "\n",
    "# example\n",
    "arr = np.array(Raw_data)\n",
    "print(arr.shape)\n",
    "print(type(arr))\n",
    "#print(arr)\n",
    "print(\"======\")\n",
    "arr_max = arr[:,0]\n",
    "print(np.nanmax(arr_max))\n",
    "\n",
    "arr_min = arr[:,0]\n",
    "print(np.nanmin(arr_min))\n",
    "\n",
    "print(arr)\n",
    "print(arr.shape)\n",
    "arr_ten = np.delete(arr, 1, 1)\n",
    "arr_ten = np.delete(arr_ten, 1, 1)\n",
    "arr_ten = np.delete(arr_ten, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(47756, 8)\n",
      "(47756, 8)\n",
      "(40000, 8)\n",
      "(40000,)\n",
      "(7756, 8)\n",
      "(7756,)\n"
     ]
    }
   ],
   "source": [
    "arr_x = arr_ten\n",
    "arr_x = np.delete(arr_x, 47756, 0)\n",
    "#print(arr_x)\n",
    "print(arr_x.shape)\n",
    "\n",
    "\n",
    "arr_y = arr_ten\n",
    "arr_y = np.delete(arr_y, 0, 0)\n",
    "#print(arr_y[0])\n",
    "print(arr_y.shape)\n",
    "\n",
    "train_x = arr_x[:40000,:]\n",
    "#train_x = np.round(train_x)\n",
    "#print(train_x)\n",
    "print(train_x.shape)\n",
    "\n",
    "train_y = arr_y[:40000,0]\n",
    "train_y = np.round(train_y)\n",
    "#print(train_y)\n",
    "print(train_y.shape)\n",
    "\n",
    "test_x = arr_x[40000:47757,:]\n",
    "#test_x = np.round(test_x)\n",
    "#print(test_x)\n",
    "print(test_x.shape)\n",
    "\n",
    "test_y = arr_y[40000:47757,0]\n",
    "test_y = np.round(test_y)\n",
    "#print(test_y)\n",
    "print(test_y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 41)\n",
      "(7756, 41)\n"
     ]
    }
   ],
   "source": [
    "##47789 split\n",
    "trainlabel_y = np.zeros([40000,41])\n",
    "i = 0\n",
    "for i in range(40000):\n",
    "    label = train_y[i]-83\n",
    "    label = int(label)\n",
    "    #print(label)\n",
    "    trainlabel_y[i][label]=1\n",
    "print(trainlabel_y.shape)\n",
    "\n",
    "testlabel_y = np.zeros([7756,41])\n",
    "i = 0\n",
    "for i in range(7756):\n",
    "    label = test_y[i]-83\n",
    "    label = int(label)\n",
    "    #print(label)\n",
    "    testlabel_y[i][label]=1\n",
    "print(testlabel_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#計算準確度\n",
    "def compute_accuracy(v_xs, v_ys):\n",
    "    global prediction\n",
    "    y_pre = sess.run(prediction, feed_dict={xs: v_xs, keep_prob: 1}) \n",
    "    correct_prediction = tf.equal(tf.argmax(y_pre,1), tf.argmax(v_ys,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    result = sess.run(accuracy, feed_dict={xs: v_xs, ys: v_ys, keep_prob: 1})\n",
    "    return result\n",
    "\n",
    "#\n",
    "# v_xs: input\n",
    "# v_ys: output\n",
    "# keep_prob是保留概率，即我们要保留的结果所占比例\n",
    "# 使输入tensor中某些元素變為0，其它没變0的元素變為原来的1/keep_prob大小！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = tf.placeholder(tf.float32, [None, 8])\n",
    "ys = tf.placeholder(tf.float32, [None, 41]) #label 41, 83 to 123\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "## fc1 layer ##\n",
    "W_fc1 = weight_variable([8, 24])\n",
    "b_fc1 = bias_variable([24])\n",
    "                                                               # [n_samples, 7, 7, 64] ->> [n_samples, 7*7*64]\n",
    "#h_pool2_flat = tf.reshape(xs, [-1, 10])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(xs, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob=0.5)\n",
    "\n",
    "## fc2 layer ##\n",
    "W_fc2 = weight_variable([24, 41])\n",
    "b_fc2 = bias_variable([41])\n",
    "prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.reduce_sum(tf.square(ys-prediction), reduction_indices=[1]))\n",
    "lr = 0.01\n",
    "train_step = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1), tf.argmax(ys,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_and_down(prediction_arr, rand_x_test, rand_y_test):\n",
    "    correctnum = 0 #預測正確數\n",
    "    std = 0 #誤差\n",
    "    for i in range(0,500):  # 設batch size=100\n",
    "        predprice=0\n",
    "        maxperd=0\n",
    "        realprice=0\n",
    "        inputprice = rand_x_test[i][0] #開盤價\n",
    "        for j in range(0, 41):\n",
    "            if rand_y_test[i][j] == 1:\n",
    "                realprice = j + 83 #真實收盤價\n",
    "            maxindex_j = np.argmax(prediction_arr[i]) #找預測label最高的機率\n",
    "            predprice = j + 83 #預測收盤價\n",
    "        \n",
    "        if inputprice < realprice and inputprice < predprice: #預測上漲且真上漲\n",
    "            correctnum = correctnum + 1\n",
    "        elif inputprice > realprice and inputprice > predprice: #預測下跌且真下跌\n",
    "            correctnum = correctnum + 1 \n",
    "        elif inputprice == realprice: #真實價格不變,不論預測漲或跌都算正確 a little tricky\n",
    "            correctnum = correctnum + 1 \n",
    "            \n",
    "        #計算誤差\n",
    "        std = std + abs(realprice - predprice) / realprice #分子取絕對值\n",
    "    #for loop 外計算正確數\n",
    "    acc = correctnum / 500\n",
    "    return acc, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training accuracy: 0.025666667148, loss: 1.617295\n",
      "epoch: 0, testing accuracy: 0.004000000190, loss: 1.698378\n",
      "rate:  0.718 std:  45.06283652898108\n",
      "==================================================\n",
      "epoch: 100, training accuracy: 0.104666665196, loss: 0.974859\n",
      "epoch: 100, testing accuracy: 0.000000000000, loss: 0.974705\n",
      "rate:  0.71 std:  45.89699284967986\n",
      "==================================================\n",
      "epoch: 200, training accuracy: 0.082000002265, loss: 0.973380\n",
      "epoch: 200, testing accuracy: 0.002000000095, loss: 0.972309\n",
      "rate:  0.716 std:  45.60231778678534\n",
      "==================================================\n",
      "epoch: 300, training accuracy: 0.101000003517, loss: 0.964966\n",
      "epoch: 300, testing accuracy: 0.197999998927, loss: 0.933797\n",
      "rate:  0.672 std:  46.46247381907517\n",
      "==================================================\n",
      "epoch: 400, training accuracy: 0.112333334982, loss: 0.964224\n",
      "epoch: 400, testing accuracy: 0.170000001788, loss: 0.943199\n",
      "rate:  0.654 std:  46.221868747551795\n",
      "==================================================\n",
      "epoch: 500, training accuracy: 0.110333330929, loss: 0.961201\n",
      "epoch: 500, testing accuracy: 0.167999997735, loss: 0.933824\n",
      "rate:  0.674 std:  46.964228898189965\n",
      "==================================================\n",
      "epoch: 600, training accuracy: 0.109333336353, loss: 0.960212\n",
      "epoch: 600, testing accuracy: 0.189999997616, loss: 0.938663\n",
      "rate:  0.668 std:  45.66467673377649\n",
      "==================================================\n",
      "epoch: 700, training accuracy: 0.117333330214, loss: 0.956757\n",
      "epoch: 700, testing accuracy: 0.186000004411, loss: 0.934402\n",
      "rate:  0.73 std:  44.80063874353294\n",
      "==================================================\n",
      "epoch: 800, training accuracy: 0.114666663110, loss: 0.958137\n",
      "epoch: 800, testing accuracy: 0.165999993682, loss: 0.943205\n",
      "rate:  0.716 std:  45.62391213914587\n",
      "==================================================\n",
      "epoch: 900, training accuracy: 0.115000002086, loss: 0.958365\n",
      "epoch: 900, testing accuracy: 0.197999998927, loss: 0.942521\n",
      "rate:  0.692 std:  45.81824888405125\n",
      "==================================================\n",
      "epoch: 1000, training accuracy: 0.110333330929, loss: 0.957796\n",
      "epoch: 1000, testing accuracy: 0.192000001669, loss: 0.939281\n",
      "rate:  0.712 std:  44.979324336563465\n",
      "==================================================\n",
      "epoch: 1100, training accuracy: 0.124666668475, loss: 0.956295\n",
      "epoch: 1100, testing accuracy: 0.175999999046, loss: 0.946542\n",
      "rate:  0.71 std:  45.485213312018736\n",
      "==================================================\n",
      "epoch: 1200, training accuracy: 0.111000001431, loss: 0.955931\n",
      "epoch: 1200, testing accuracy: 0.178000003099, loss: 0.934182\n",
      "rate:  0.74 std:  44.996889538308544\n",
      "==================================================\n",
      "epoch: 1300, training accuracy: 0.126666665077, loss: 0.954928\n",
      "epoch: 1300, testing accuracy: 0.208000004292, loss: 0.939470\n",
      "rate:  0.73 std:  44.99033276097849\n",
      "==================================================\n",
      "epoch: 1400, training accuracy: 0.136999994516, loss: 0.953853\n",
      "epoch: 1400, testing accuracy: 0.194000005722, loss: 0.941005\n",
      "rate:  0.68 std:  47.521261348599104\n",
      "==================================================\n",
      "epoch: 1500, training accuracy: 0.126333326101, loss: 0.954055\n",
      "epoch: 1500, testing accuracy: 0.209999993443, loss: 0.929377\n",
      "rate:  0.704 std:  45.27967010002623\n",
      "==================================================\n",
      "epoch: 1600, training accuracy: 0.143333330750, loss: 0.951280\n",
      "epoch: 1600, testing accuracy: 0.195999994874, loss: 0.936387\n",
      "rate:  0.7 std:  47.047342977171674\n",
      "==================================================\n",
      "epoch: 1700, training accuracy: 0.134000003338, loss: 0.952151\n",
      "epoch: 1700, testing accuracy: 0.175999999046, loss: 0.939992\n",
      "rate:  0.742 std:  47.22333940564698\n",
      "==================================================\n",
      "epoch: 1800, training accuracy: 0.128999993205, loss: 0.952085\n",
      "epoch: 1800, testing accuracy: 0.195999994874, loss: 0.947599\n",
      "rate:  0.664 std:  46.41452487543229\n",
      "==================================================\n",
      "epoch: 1900, training accuracy: 0.131666660309, loss: 0.951954\n",
      "epoch: 1900, testing accuracy: 0.184000000358, loss: 0.942848\n",
      "rate:  0.698 std:  46.13674863202817\n",
      "==================================================\n",
      "epoch: 2000, training accuracy: 0.144666671753, loss: 0.950066\n",
      "epoch: 2000, testing accuracy: 0.167999997735, loss: 0.938293\n",
      "rate:  0.7 std:  46.68062154888797\n",
      "==================================================\n",
      "epoch: 2100, training accuracy: 0.141000002623, loss: 0.951818\n",
      "epoch: 2100, testing accuracy: 0.186000004411, loss: 0.933457\n",
      "rate:  0.716 std:  46.016405757659335\n",
      "==================================================\n",
      "epoch: 2200, training accuracy: 0.147333338857, loss: 0.949496\n",
      "epoch: 2200, testing accuracy: 0.195999994874, loss: 0.940372\n",
      "rate:  0.724 std:  44.440089546698964\n",
      "==================================================\n",
      "epoch: 2300, training accuracy: 0.144999995828, loss: 0.948609\n",
      "epoch: 2300, testing accuracy: 0.178000003099, loss: 0.948419\n",
      "rate:  0.728 std:  46.00251396886049\n",
      "==================================================\n",
      "epoch: 2400, training accuracy: 0.142666667700, loss: 0.948325\n",
      "epoch: 2400, testing accuracy: 0.184000000358, loss: 0.945255\n",
      "rate:  0.71 std:  46.034924126539856\n",
      "==================================================\n",
      "epoch: 2500, training accuracy: 0.154333338141, loss: 0.945682\n",
      "epoch: 2500, testing accuracy: 0.150000005960, loss: 0.936383\n",
      "rate:  0.724 std:  47.10154522659031\n",
      "==================================================\n",
      "epoch: 2600, training accuracy: 0.145999997854, loss: 0.948226\n",
      "epoch: 2600, testing accuracy: 0.184000000358, loss: 0.937907\n",
      "rate:  0.696 std:  47.02047815739711\n",
      "==================================================\n",
      "epoch: 2700, training accuracy: 0.142000004649, loss: 0.947806\n",
      "epoch: 2700, testing accuracy: 0.180000007153, loss: 0.937060\n",
      "rate:  0.71 std:  45.95714245740457\n",
      "==================================================\n",
      "epoch: 2800, training accuracy: 0.150999993086, loss: 0.944467\n",
      "epoch: 2800, testing accuracy: 0.184000000358, loss: 0.941235\n",
      "rate:  0.708 std:  44.95806572138106\n",
      "==================================================\n",
      "epoch: 2900, training accuracy: 0.143000006676, loss: 0.944348\n",
      "epoch: 2900, testing accuracy: 0.186000004411, loss: 0.936613\n",
      "rate:  0.698 std:  46.71173601849882\n",
      "==================================================\n",
      "epoch: 3000, training accuracy: 0.150000005960, loss: 0.946714\n",
      "epoch: 3000, testing accuracy: 0.200000002980, loss: 0.951411\n",
      "rate:  0.71 std:  45.05848065693285\n",
      "==================================================\n",
      "epoch: 3100, training accuracy: 0.143333330750, loss: 0.946071\n",
      "epoch: 3100, testing accuracy: 0.129999995232, loss: 0.938790\n",
      "rate:  0.702 std:  47.073969710394216\n",
      "==================================================\n",
      "epoch: 3200, training accuracy: 0.151333332062, loss: 0.942911\n",
      "epoch: 3200, testing accuracy: 0.186000004411, loss: 0.933817\n",
      "rate:  0.726 std:  45.40840548065768\n",
      "==================================================\n",
      "epoch: 3300, training accuracy: 0.144999995828, loss: 0.942972\n",
      "epoch: 3300, testing accuracy: 0.153999999166, loss: 0.946612\n",
      "rate:  0.682 std:  45.685970188623685\n",
      "==================================================\n",
      "epoch: 3400, training accuracy: 0.151666671038, loss: 0.944893\n",
      "epoch: 3400, testing accuracy: 0.180000007153, loss: 0.946475\n",
      "rate:  0.688 std:  45.9267261789751\n",
      "==================================================\n",
      "epoch: 3500, training accuracy: 0.149000003934, loss: 0.941151\n",
      "epoch: 3500, testing accuracy: 0.165999993682, loss: 0.947027\n",
      "rate:  0.728 std:  45.64006579449725\n",
      "==================================================\n",
      "epoch: 3600, training accuracy: 0.145666673779, loss: 0.941361\n",
      "epoch: 3600, testing accuracy: 0.167999997735, loss: 0.949311\n",
      "rate:  0.726 std:  46.7438482421503\n",
      "==================================================\n",
      "epoch: 3700, training accuracy: 0.167333334684, loss: 0.940292\n",
      "epoch: 3700, testing accuracy: 0.195999994874, loss: 0.945136\n",
      "rate:  0.68 std:  45.585099605407756\n",
      "==================================================\n",
      "epoch: 3800, training accuracy: 0.151666671038, loss: 0.942488\n",
      "epoch: 3800, testing accuracy: 0.195999994874, loss: 0.953703\n",
      "rate:  0.702 std:  45.35299489429016\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3900, training accuracy: 0.148333325982, loss: 0.944360\n",
      "epoch: 3900, testing accuracy: 0.202000007033, loss: 0.931394\n",
      "rate:  0.65 std:  44.608898884175154\n",
      "==================================================\n",
      "epoch: 4000, training accuracy: 0.147666662931, loss: 0.940156\n",
      "epoch: 4000, testing accuracy: 0.172000005841, loss: 0.947222\n",
      "rate:  0.68 std:  45.87086486641805\n",
      "==================================================\n",
      "epoch: 4100, training accuracy: 0.157666668296, loss: 0.938755\n",
      "epoch: 4100, testing accuracy: 0.156000003219, loss: 0.945631\n",
      "rate:  0.688 std:  48.74319896928797\n",
      "==================================================\n",
      "epoch: 4200, training accuracy: 0.161666661501, loss: 0.940222\n",
      "epoch: 4200, testing accuracy: 0.192000001669, loss: 0.948127\n",
      "rate:  0.692 std:  46.178753822438836\n",
      "==================================================\n",
      "epoch: 4300, training accuracy: 0.145999997854, loss: 0.940519\n",
      "epoch: 4300, testing accuracy: 0.153999999166, loss: 0.947270\n",
      "rate:  0.694 std:  45.74569410871904\n",
      "==================================================\n",
      "epoch: 4400, training accuracy: 0.148666664958, loss: 0.942264\n",
      "epoch: 4400, testing accuracy: 0.192000001669, loss: 0.946636\n",
      "rate:  0.734 std:  46.14717007890029\n",
      "==================================================\n",
      "epoch: 4500, training accuracy: 0.136333331466, loss: 0.942374\n",
      "epoch: 4500, testing accuracy: 0.192000001669, loss: 0.944123\n",
      "rate:  0.658 std:  45.361671623437516\n",
      "==================================================\n",
      "epoch: 4600, training accuracy: 0.154333338141, loss: 0.941053\n",
      "epoch: 4600, testing accuracy: 0.195999994874, loss: 0.944158\n",
      "rate:  0.7 std:  44.40395126697477\n",
      "==================================================\n",
      "epoch: 4700, training accuracy: 0.151333332062, loss: 0.940573\n",
      "epoch: 4700, testing accuracy: 0.189999997616, loss: 0.951471\n",
      "rate:  0.7 std:  47.828415985286554\n",
      "==================================================\n",
      "epoch: 4800, training accuracy: 0.149666666985, loss: 0.939255\n",
      "epoch: 4800, testing accuracy: 0.194000005722, loss: 0.946662\n",
      "rate:  0.734 std:  45.732274626833906\n",
      "==================================================\n",
      "epoch: 4900, training accuracy: 0.147666662931, loss: 0.939919\n",
      "epoch: 4900, testing accuracy: 0.181999996305, loss: 0.945631\n",
      "rate:  0.728 std:  47.46905526195777\n",
      "==================================================\n",
      "epoch: 5000, training accuracy: 0.151333332062, loss: 0.939884\n",
      "epoch: 5000, testing accuracy: 0.189999997616, loss: 0.943743\n",
      "rate:  0.702 std:  44.85863517443519\n",
      "==================================================\n",
      "epoch: 5100, training accuracy: 0.148666664958, loss: 0.939535\n",
      "epoch: 5100, testing accuracy: 0.178000003099, loss: 0.949250\n",
      "rate:  0.674 std:  45.39437727894192\n",
      "==================================================\n",
      "epoch: 5200, training accuracy: 0.147666662931, loss: 0.940595\n",
      "epoch: 5200, testing accuracy: 0.143999993801, loss: 0.954294\n",
      "rate:  0.694 std:  46.42864857360847\n",
      "==================================================\n",
      "epoch: 5300, training accuracy: 0.149000003934, loss: 0.938179\n",
      "epoch: 5300, testing accuracy: 0.200000002980, loss: 0.938211\n",
      "rate:  0.69 std:  43.788859712446346\n",
      "==================================================\n",
      "epoch: 5400, training accuracy: 0.155666664243, loss: 0.938842\n",
      "epoch: 5400, testing accuracy: 0.167999997735, loss: 0.949541\n",
      "rate:  0.704 std:  48.308454282896854\n",
      "==================================================\n",
      "epoch: 5500, training accuracy: 0.149333328009, loss: 0.938785\n",
      "epoch: 5500, testing accuracy: 0.164000004530, loss: 0.949632\n",
      "rate:  0.698 std:  44.31895401252444\n",
      "==================================================\n",
      "epoch: 5600, training accuracy: 0.137999996543, loss: 0.938968\n",
      "epoch: 5600, testing accuracy: 0.175999999046, loss: 0.948692\n",
      "rate:  0.706 std:  46.31272744683783\n",
      "==================================================\n",
      "epoch: 5700, training accuracy: 0.141666665673, loss: 0.939971\n",
      "epoch: 5700, testing accuracy: 0.156000003219, loss: 0.942690\n",
      "rate:  0.72 std:  46.71389937960608\n",
      "==================================================\n",
      "epoch: 5800, training accuracy: 0.152999997139, loss: 0.936978\n",
      "epoch: 5800, testing accuracy: 0.178000003099, loss: 0.944785\n",
      "rate:  0.688 std:  45.43566860952899\n",
      "==================================================\n",
      "epoch: 5900, training accuracy: 0.151333332062, loss: 0.938797\n",
      "epoch: 5900, testing accuracy: 0.159999996424, loss: 0.944512\n",
      "rate:  0.708 std:  44.878523628019934\n",
      "==================================================\n",
      "epoch: 6000, training accuracy: 0.150666669011, loss: 0.939329\n",
      "epoch: 6000, testing accuracy: 0.162000000477, loss: 0.948034\n",
      "rate:  0.694 std:  45.966849363687594\n",
      "==================================================\n",
      "epoch: 6100, training accuracy: 0.155000001192, loss: 0.937721\n",
      "epoch: 6100, testing accuracy: 0.202000007033, loss: 0.943220\n",
      "rate:  0.704 std:  45.87698485590665\n",
      "==================================================\n",
      "epoch: 6200, training accuracy: 0.149666666985, loss: 0.939876\n",
      "epoch: 6200, testing accuracy: 0.167999997735, loss: 0.952563\n",
      "rate:  0.686 std:  44.85132023027051\n",
      "==================================================\n",
      "epoch: 6300, training accuracy: 0.143000006676, loss: 0.937590\n",
      "epoch: 6300, testing accuracy: 0.180000007153, loss: 0.952717\n",
      "rate:  0.72 std:  48.10414534077367\n",
      "==================================================\n",
      "epoch: 6400, training accuracy: 0.138666659594, loss: 0.939075\n",
      "epoch: 6400, testing accuracy: 0.202000007033, loss: 0.949769\n",
      "rate:  0.712 std:  45.471118611480904\n",
      "==================================================\n",
      "epoch: 6500, training accuracy: 0.142666667700, loss: 0.938308\n",
      "epoch: 6500, testing accuracy: 0.195999994874, loss: 0.948554\n",
      "rate:  0.71 std:  45.0854667697255\n",
      "==================================================\n",
      "epoch: 6600, training accuracy: 0.148000001907, loss: 0.936013\n",
      "epoch: 6600, testing accuracy: 0.158000007272, loss: 0.948169\n",
      "rate:  0.688 std:  46.63053186443358\n",
      "==================================================\n",
      "epoch: 6700, training accuracy: 0.143333330750, loss: 0.940012\n",
      "epoch: 6700, testing accuracy: 0.187999993563, loss: 0.951288\n",
      "rate:  0.692 std:  45.249344715283854\n",
      "==================================================\n",
      "epoch: 6800, training accuracy: 0.145333334804, loss: 0.937011\n",
      "epoch: 6800, testing accuracy: 0.184000000358, loss: 0.947277\n",
      "rate:  0.688 std:  46.254569718851116\n",
      "==================================================\n",
      "epoch: 6900, training accuracy: 0.147666662931, loss: 0.938487\n",
      "epoch: 6900, testing accuracy: 0.206000000238, loss: 0.946544\n",
      "rate:  0.666 std:  45.69013210397504\n",
      "==================================================\n",
      "epoch: 7000, training accuracy: 0.149666666985, loss: 0.936943\n",
      "epoch: 7000, testing accuracy: 0.202000007033, loss: 0.945478\n",
      "rate:  0.672 std:  45.02048052671665\n",
      "==================================================\n",
      "epoch: 7100, training accuracy: 0.142333328724, loss: 0.938530\n",
      "epoch: 7100, testing accuracy: 0.137999996543, loss: 0.954334\n",
      "rate:  0.676 std:  46.387174924387296\n",
      "==================================================\n",
      "epoch: 7200, training accuracy: 0.155333340168, loss: 0.937353\n",
      "epoch: 7200, testing accuracy: 0.162000000477, loss: 0.945769\n",
      "rate:  0.702 std:  46.55254922508675\n",
      "==================================================\n",
      "epoch: 7300, training accuracy: 0.138333335519, loss: 0.939910\n",
      "epoch: 7300, testing accuracy: 0.128000006080, loss: 0.946010\n",
      "rate:  0.746 std:  47.03928458495247\n",
      "==================================================\n",
      "epoch: 7400, training accuracy: 0.127666667104, loss: 0.942715\n",
      "epoch: 7400, testing accuracy: 0.184000000358, loss: 0.947631\n",
      "rate:  0.718 std:  47.4800360198793\n",
      "==================================================\n",
      "epoch: 7500, training accuracy: 0.145666673779, loss: 0.937873\n",
      "epoch: 7500, testing accuracy: 0.178000003099, loss: 0.939516\n",
      "rate:  0.712 std:  44.212066647674575\n",
      "==================================================\n",
      "epoch: 7600, training accuracy: 0.150000005960, loss: 0.939177\n",
      "epoch: 7600, testing accuracy: 0.156000003219, loss: 0.954937\n",
      "rate:  0.7 std:  46.439536257443216\n",
      "==================================================\n",
      "epoch: 7700, training accuracy: 0.149000003934, loss: 0.936268\n",
      "epoch: 7700, testing accuracy: 0.192000001669, loss: 0.949755\n",
      "rate:  0.708 std:  45.89331835367834\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7800, training accuracy: 0.157666668296, loss: 0.934791\n",
      "epoch: 7800, testing accuracy: 0.187999993563, loss: 0.949355\n",
      "rate:  0.696 std:  45.07473015044221\n",
      "==================================================\n",
      "epoch: 7900, training accuracy: 0.144333332777, loss: 0.935140\n",
      "epoch: 7900, testing accuracy: 0.175999999046, loss: 0.945163\n",
      "rate:  0.662 std:  45.95187097197365\n",
      "==================================================\n",
      "epoch: 8000, training accuracy: 0.147333338857, loss: 0.939237\n",
      "epoch: 8000, testing accuracy: 0.192000001669, loss: 0.946154\n",
      "rate:  0.684 std:  45.562640343857886\n",
      "==================================================\n",
      "epoch: 8100, training accuracy: 0.142666667700, loss: 0.937396\n",
      "epoch: 8100, testing accuracy: 0.162000000477, loss: 0.943559\n",
      "rate:  0.694 std:  45.51376275760343\n",
      "==================================================\n",
      "epoch: 8200, training accuracy: 0.154666662216, loss: 0.937573\n",
      "epoch: 8200, testing accuracy: 0.158000007272, loss: 0.951473\n",
      "rate:  0.684 std:  46.64379109573434\n",
      "==================================================\n",
      "epoch: 8300, training accuracy: 0.159999996424, loss: 0.934603\n",
      "epoch: 8300, testing accuracy: 0.195999994874, loss: 0.950925\n",
      "rate:  0.708 std:  45.36499504017219\n",
      "==================================================\n",
      "epoch: 8400, training accuracy: 0.152999997139, loss: 0.934655\n",
      "epoch: 8400, testing accuracy: 0.165999993682, loss: 0.939134\n",
      "rate:  0.666 std:  45.48202227034184\n",
      "==================================================\n",
      "epoch: 8500, training accuracy: 0.139333337545, loss: 0.939208\n",
      "epoch: 8500, testing accuracy: 0.164000004530, loss: 0.955122\n",
      "rate:  0.716 std:  45.29253569318438\n",
      "==================================================\n",
      "epoch: 8600, training accuracy: 0.160999998450, loss: 0.933438\n",
      "epoch: 8600, testing accuracy: 0.151999995112, loss: 0.956234\n",
      "rate:  0.68 std:  46.639101942508425\n",
      "==================================================\n",
      "epoch: 8700, training accuracy: 0.143333330750, loss: 0.938348\n",
      "epoch: 8700, testing accuracy: 0.170000001788, loss: 0.949752\n",
      "rate:  0.694 std:  46.761498012498734\n",
      "==================================================\n",
      "epoch: 8800, training accuracy: 0.144333332777, loss: 0.938179\n",
      "epoch: 8800, testing accuracy: 0.158000007272, loss: 0.944819\n",
      "rate:  0.676 std:  44.228758446122434\n",
      "==================================================\n",
      "epoch: 8900, training accuracy: 0.145333334804, loss: 0.937020\n",
      "epoch: 8900, testing accuracy: 0.173999994993, loss: 0.945508\n",
      "rate:  0.686 std:  45.707013554747775\n",
      "==================================================\n",
      "epoch: 9000, training accuracy: 0.147333338857, loss: 0.937547\n",
      "epoch: 9000, testing accuracy: 0.181999996305, loss: 0.946926\n",
      "rate:  0.698 std:  46.28478395228952\n",
      "==================================================\n",
      "epoch: 9100, training accuracy: 0.140333339572, loss: 0.937579\n",
      "epoch: 9100, testing accuracy: 0.180000007153, loss: 0.946462\n",
      "rate:  0.706 std:  46.32260415484917\n",
      "==================================================\n",
      "epoch: 9200, training accuracy: 0.140666663647, loss: 0.939370\n",
      "epoch: 9200, testing accuracy: 0.162000000477, loss: 0.946235\n",
      "rate:  0.7 std:  45.76636360378454\n",
      "==================================================\n",
      "epoch: 9300, training accuracy: 0.138333335519, loss: 0.939116\n",
      "epoch: 9300, testing accuracy: 0.175999999046, loss: 0.946360\n",
      "rate:  0.676 std:  45.402407939490146\n",
      "==================================================\n",
      "epoch: 9400, training accuracy: 0.140000000596, loss: 0.937382\n",
      "epoch: 9400, testing accuracy: 0.172000005841, loss: 0.951090\n",
      "rate:  0.724 std:  45.454424450697765\n",
      "==================================================\n",
      "epoch: 9500, training accuracy: 0.157666668296, loss: 0.935122\n",
      "epoch: 9500, testing accuracy: 0.180000007153, loss: 0.957493\n",
      "rate:  0.768 std:  45.556634800735964\n",
      "==================================================\n",
      "epoch: 9600, training accuracy: 0.147333338857, loss: 0.935798\n",
      "epoch: 9600, testing accuracy: 0.159999996424, loss: 0.950782\n",
      "rate:  0.712 std:  45.74099267974778\n",
      "==================================================\n",
      "epoch: 9700, training accuracy: 0.138666659594, loss: 0.939220\n",
      "epoch: 9700, testing accuracy: 0.186000004411, loss: 0.944732\n",
      "rate:  0.692 std:  46.55109913086067\n",
      "==================================================\n",
      "epoch: 9800, training accuracy: 0.150666669011, loss: 0.935761\n",
      "epoch: 9800, testing accuracy: 0.172000005841, loss: 0.945408\n",
      "rate:  0.702 std:  45.68379428682918\n",
      "==================================================\n",
      "epoch: 9900, training accuracy: 0.150000005960, loss: 0.934574\n",
      "epoch: 9900, testing accuracy: 0.167999997735, loss: 0.944631\n",
      "rate:  0.698 std:  45.57727444534444\n",
      "==================================================\n",
      "epoch: 10000, training accuracy: 0.141000002623, loss: 0.938929\n",
      "epoch: 10000, testing accuracy: 0.175999999046, loss: 0.959315\n",
      "rate:  0.696 std:  47.43460840666859\n",
      "==================================================\n",
      "epoch: 10100, training accuracy: 0.152333334088, loss: 0.936391\n",
      "epoch: 10100, testing accuracy: 0.181999996305, loss: 0.944748\n",
      "rate:  0.672 std:  45.47746505854725\n",
      "==================================================\n",
      "epoch: 10200, training accuracy: 0.154666662216, loss: 0.935200\n",
      "epoch: 10200, testing accuracy: 0.165999993682, loss: 0.948423\n",
      "rate:  0.696 std:  46.069781333813104\n",
      "==================================================\n",
      "epoch: 10300, training accuracy: 0.136666670442, loss: 0.938067\n",
      "epoch: 10300, testing accuracy: 0.202000007033, loss: 0.944407\n",
      "rate:  0.696 std:  46.00918068215116\n",
      "==================================================\n",
      "epoch: 10400, training accuracy: 0.146999999881, loss: 0.934820\n",
      "epoch: 10400, testing accuracy: 0.172000005841, loss: 0.947447\n",
      "rate:  0.69 std:  45.258749150224325\n",
      "==================================================\n",
      "epoch: 10500, training accuracy: 0.144333332777, loss: 0.936937\n",
      "epoch: 10500, testing accuracy: 0.195999994874, loss: 0.947865\n",
      "rate:  0.708 std:  47.213164494213494\n",
      "==================================================\n",
      "epoch: 10600, training accuracy: 0.146666660905, loss: 0.937330\n",
      "epoch: 10600, testing accuracy: 0.180000007153, loss: 0.943052\n",
      "rate:  0.71 std:  45.48992343545796\n",
      "==================================================\n",
      "epoch: 10700, training accuracy: 0.151333332062, loss: 0.934069\n",
      "epoch: 10700, testing accuracy: 0.181999996305, loss: 0.947899\n",
      "rate:  0.692 std:  46.04734223101481\n",
      "==================================================\n",
      "epoch: 10800, training accuracy: 0.144333332777, loss: 0.935380\n",
      "epoch: 10800, testing accuracy: 0.194000005722, loss: 0.945889\n",
      "rate:  0.684 std:  45.923505331482204\n",
      "==================================================\n",
      "epoch: 10900, training accuracy: 0.134666666389, loss: 0.936732\n",
      "epoch: 10900, testing accuracy: 0.203999996185, loss: 0.947727\n",
      "rate:  0.702 std:  46.511653302131\n",
      "==================================================\n",
      "epoch: 11000, training accuracy: 0.142333328724, loss: 0.938735\n",
      "epoch: 11000, testing accuracy: 0.151999995112, loss: 0.941629\n",
      "rate:  0.732 std:  46.01364452066099\n",
      "==================================================\n",
      "epoch: 11100, training accuracy: 0.142000004649, loss: 0.935179\n",
      "epoch: 11100, testing accuracy: 0.170000001788, loss: 0.955560\n",
      "rate:  0.692 std:  44.77178088461538\n",
      "==================================================\n",
      "epoch: 11200, training accuracy: 0.151666671038, loss: 0.935100\n",
      "epoch: 11200, testing accuracy: 0.180000007153, loss: 0.956154\n",
      "rate:  0.704 std:  46.67254680188116\n",
      "==================================================\n",
      "epoch: 11300, training accuracy: 0.148666664958, loss: 0.939095\n",
      "epoch: 11300, testing accuracy: 0.194000005722, loss: 0.950675\n",
      "rate:  0.714 std:  46.24259059687481\n",
      "==================================================\n",
      "epoch: 11400, training accuracy: 0.149333328009, loss: 0.936548\n",
      "epoch: 11400, testing accuracy: 0.158000007272, loss: 0.950304\n",
      "rate:  0.734 std:  44.20626575421342\n",
      "==================================================\n",
      "epoch: 11500, training accuracy: 0.148666664958, loss: 0.937650\n",
      "epoch: 11500, testing accuracy: 0.181999996305, loss: 0.949300\n",
      "rate:  0.67 std:  47.8230368695359\n",
      "==================================================\n",
      "epoch: 11600, training accuracy: 0.144999995828, loss: 0.935112\n",
      "epoch: 11600, testing accuracy: 0.165999993682, loss: 0.950441\n",
      "rate:  0.698 std:  47.12574846553758\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11700, training accuracy: 0.154333338141, loss: 0.932845\n",
      "epoch: 11700, testing accuracy: 0.178000003099, loss: 0.935848\n",
      "rate:  0.728 std:  44.255496678059735\n",
      "==================================================\n",
      "epoch: 11800, training accuracy: 0.152666673064, loss: 0.936545\n",
      "epoch: 11800, testing accuracy: 0.187999993563, loss: 0.943805\n",
      "rate:  0.704 std:  45.056402908493624\n",
      "==================================================\n",
      "epoch: 11900, training accuracy: 0.159999996424, loss: 0.934649\n",
      "epoch: 11900, testing accuracy: 0.192000001669, loss: 0.943640\n",
      "rate:  0.716 std:  45.98016891902855\n",
      "==================================================\n",
      "epoch: 12000, training accuracy: 0.150333330035, loss: 0.936660\n",
      "epoch: 12000, testing accuracy: 0.173999994993, loss: 0.951647\n",
      "rate:  0.704 std:  45.963965543722026\n",
      "==================================================\n",
      "epoch: 12100, training accuracy: 0.141000002623, loss: 0.936208\n",
      "epoch: 12100, testing accuracy: 0.194000005722, loss: 0.951799\n",
      "rate:  0.672 std:  45.21417950119125\n",
      "==================================================\n",
      "epoch: 12200, training accuracy: 0.152666673064, loss: 0.934530\n",
      "epoch: 12200, testing accuracy: 0.195999994874, loss: 0.956091\n",
      "rate:  0.682 std:  45.18675143138111\n",
      "==================================================\n",
      "epoch: 12300, training accuracy: 0.149666666985, loss: 0.938227\n",
      "epoch: 12300, testing accuracy: 0.194000005722, loss: 0.949548\n",
      "rate:  0.736 std:  45.67286423657113\n",
      "==================================================\n",
      "epoch: 12400, training accuracy: 0.151666671038, loss: 0.934845\n",
      "epoch: 12400, testing accuracy: 0.192000001669, loss: 0.946496\n",
      "rate:  0.69 std:  46.01409153615215\n",
      "==================================================\n",
      "epoch: 12500, training accuracy: 0.133333340287, loss: 0.937553\n",
      "epoch: 12500, testing accuracy: 0.187999993563, loss: 0.938739\n",
      "rate:  0.69 std:  46.00446744813243\n",
      "==================================================\n",
      "epoch: 12600, training accuracy: 0.141333326697, loss: 0.937152\n",
      "epoch: 12600, testing accuracy: 0.181999996305, loss: 0.952578\n",
      "rate:  0.718 std:  45.61761135415777\n",
      "==================================================\n",
      "epoch: 12700, training accuracy: 0.150333330035, loss: 0.935151\n",
      "epoch: 12700, testing accuracy: 0.197999998927, loss: 0.952387\n",
      "rate:  0.694 std:  47.17598842964627\n",
      "==================================================\n",
      "epoch: 12800, training accuracy: 0.149666666985, loss: 0.937593\n",
      "epoch: 12800, testing accuracy: 0.170000001788, loss: 0.942154\n",
      "rate:  0.698 std:  45.25162574602112\n",
      "==================================================\n",
      "epoch: 12900, training accuracy: 0.151666671038, loss: 0.936676\n",
      "epoch: 12900, testing accuracy: 0.175999999046, loss: 0.949387\n",
      "rate:  0.718 std:  47.840331335205406\n",
      "==================================================\n",
      "epoch: 13000, training accuracy: 0.149000003934, loss: 0.935081\n",
      "epoch: 13000, testing accuracy: 0.200000002980, loss: 0.947171\n",
      "rate:  0.704 std:  45.024876276873506\n",
      "==================================================\n",
      "epoch: 13100, training accuracy: 0.152666673064, loss: 0.934426\n",
      "epoch: 13100, testing accuracy: 0.195999994874, loss: 0.951963\n",
      "rate:  0.724 std:  46.918212518517294\n",
      "==================================================\n",
      "epoch: 13200, training accuracy: 0.139333337545, loss: 0.934547\n",
      "epoch: 13200, testing accuracy: 0.158000007272, loss: 0.955275\n",
      "rate:  0.748 std:  45.76616992492808\n",
      "==================================================\n",
      "epoch: 13300, training accuracy: 0.150333330035, loss: 0.933240\n",
      "epoch: 13300, testing accuracy: 0.172000005841, loss: 0.949146\n",
      "rate:  0.698 std:  47.10368467645511\n",
      "==================================================\n",
      "epoch: 13400, training accuracy: 0.145333334804, loss: 0.938136\n",
      "epoch: 13400, testing accuracy: 0.186000004411, loss: 0.946837\n",
      "rate:  0.688 std:  45.885533115681014\n",
      "==================================================\n",
      "epoch: 13500, training accuracy: 0.145333334804, loss: 0.935043\n",
      "epoch: 13500, testing accuracy: 0.170000001788, loss: 0.953404\n",
      "rate:  0.688 std:  45.7418913245617\n",
      "==================================================\n",
      "epoch: 13600, training accuracy: 0.150333330035, loss: 0.934760\n",
      "epoch: 13600, testing accuracy: 0.184000000358, loss: 0.945224\n",
      "rate:  0.678 std:  46.942522870126915\n",
      "==================================================\n",
      "epoch: 13700, training accuracy: 0.147666662931, loss: 0.936491\n",
      "epoch: 13700, testing accuracy: 0.143999993801, loss: 0.953711\n",
      "rate:  0.704 std:  44.80041467674138\n",
      "==================================================\n",
      "epoch: 13800, training accuracy: 0.148333325982, loss: 0.934213\n",
      "epoch: 13800, testing accuracy: 0.173999994993, loss: 0.939030\n",
      "rate:  0.682 std:  47.27147155463106\n",
      "==================================================\n",
      "epoch: 13900, training accuracy: 0.143666669726, loss: 0.934813\n",
      "epoch: 13900, testing accuracy: 0.195999994874, loss: 0.941934\n",
      "rate:  0.71 std:  44.42848827334255\n",
      "==================================================\n",
      "epoch: 14000, training accuracy: 0.139666661620, loss: 0.936597\n",
      "epoch: 14000, testing accuracy: 0.162000000477, loss: 0.947738\n",
      "rate:  0.72 std:  45.75900018727956\n",
      "==================================================\n",
      "epoch: 14100, training accuracy: 0.150999993086, loss: 0.934678\n",
      "epoch: 14100, testing accuracy: 0.167999997735, loss: 0.946720\n",
      "rate:  0.664 std:  47.61637875734768\n",
      "==================================================\n",
      "epoch: 14200, training accuracy: 0.150666669011, loss: 0.934947\n",
      "epoch: 14200, testing accuracy: 0.181999996305, loss: 0.943399\n",
      "rate:  0.702 std:  45.569759767280054\n",
      "==================================================\n",
      "epoch: 14300, training accuracy: 0.136333331466, loss: 0.937979\n",
      "epoch: 14300, testing accuracy: 0.194000005722, loss: 0.943410\n",
      "rate:  0.732 std:  45.0520339221876\n",
      "==================================================\n",
      "epoch: 14400, training accuracy: 0.148666664958, loss: 0.935947\n",
      "epoch: 14400, testing accuracy: 0.164000004530, loss: 0.952399\n",
      "rate:  0.704 std:  47.62270716003252\n",
      "==================================================\n",
      "epoch: 14500, training accuracy: 0.154333338141, loss: 0.933975\n",
      "epoch: 14500, testing accuracy: 0.153999999166, loss: 0.946072\n",
      "rate:  0.706 std:  45.93334078765838\n",
      "==================================================\n",
      "epoch: 14600, training accuracy: 0.145999997854, loss: 0.936294\n",
      "epoch: 14600, testing accuracy: 0.170000001788, loss: 0.945632\n",
      "rate:  0.672 std:  45.70512331034995\n",
      "==================================================\n",
      "epoch: 14700, training accuracy: 0.144666671753, loss: 0.936414\n",
      "epoch: 14700, testing accuracy: 0.206000000238, loss: 0.941364\n",
      "rate:  0.718 std:  45.489483872573025\n",
      "==================================================\n",
      "epoch: 14800, training accuracy: 0.153666660190, loss: 0.935500\n",
      "epoch: 14800, testing accuracy: 0.165999993682, loss: 0.945604\n",
      "rate:  0.712 std:  45.76856456357929\n",
      "==================================================\n",
      "epoch: 14900, training accuracy: 0.150666669011, loss: 0.933732\n",
      "epoch: 14900, testing accuracy: 0.186000004411, loss: 0.944816\n",
      "rate:  0.676 std:  45.58724316651698\n",
      "==================================================\n",
      "epoch: 15000, training accuracy: 0.143333330750, loss: 0.936635\n",
      "epoch: 15000, testing accuracy: 0.197999998927, loss: 0.937581\n",
      "rate:  0.738 std:  47.04843934766041\n",
      "==================================================\n",
      "epoch: 15100, training accuracy: 0.152333334088, loss: 0.934870\n",
      "epoch: 15100, testing accuracy: 0.167999997735, loss: 0.950019\n",
      "rate:  0.736 std:  46.631694141407685\n",
      "==================================================\n",
      "epoch: 15200, training accuracy: 0.136999994516, loss: 0.937184\n",
      "epoch: 15200, testing accuracy: 0.192000001669, loss: 0.939333\n",
      "rate:  0.664 std:  47.52418212301451\n",
      "==================================================\n",
      "epoch: 15300, training accuracy: 0.149333328009, loss: 0.936589\n",
      "epoch: 15300, testing accuracy: 0.181999996305, loss: 0.943778\n",
      "rate:  0.656 std:  46.430761727237396\n",
      "==================================================\n",
      "epoch: 15400, training accuracy: 0.150999993086, loss: 0.934093\n",
      "epoch: 15400, testing accuracy: 0.181999996305, loss: 0.940318\n",
      "rate:  0.708 std:  44.148012769653285\n",
      "==================================================\n",
      "epoch: 15500, training accuracy: 0.150333330035, loss: 0.935933\n",
      "epoch: 15500, testing accuracy: 0.216000005603, loss: 0.947668\n",
      "rate:  0.736 std:  44.43373692858478\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15600, training accuracy: 0.143333330750, loss: 0.934770\n",
      "epoch: 15600, testing accuracy: 0.165999993682, loss: 0.943811\n",
      "rate:  0.72 std:  45.63939855463897\n",
      "==================================================\n",
      "epoch: 15700, training accuracy: 0.144333332777, loss: 0.935414\n",
      "epoch: 15700, testing accuracy: 0.158000007272, loss: 0.946154\n",
      "rate:  0.706 std:  46.341258228570105\n",
      "==================================================\n",
      "epoch: 15800, training accuracy: 0.139333337545, loss: 0.935310\n",
      "epoch: 15800, testing accuracy: 0.145999997854, loss: 0.950883\n",
      "rate:  0.712 std:  46.32490666253128\n",
      "==================================================\n",
      "epoch: 15900, training accuracy: 0.149333328009, loss: 0.937001\n",
      "epoch: 15900, testing accuracy: 0.165999993682, loss: 0.946401\n",
      "rate:  0.714 std:  45.763049828798096\n",
      "==================================================\n",
      "epoch: 16000, training accuracy: 0.156000003219, loss: 0.935020\n",
      "epoch: 16000, testing accuracy: 0.170000001788, loss: 0.943438\n",
      "rate:  0.668 std:  45.70348717722712\n",
      "==================================================\n",
      "epoch: 16100, training accuracy: 0.143000006676, loss: 0.937752\n",
      "epoch: 16100, testing accuracy: 0.197999998927, loss: 0.937836\n",
      "rate:  0.698 std:  45.59381550923442\n",
      "==================================================\n",
      "epoch: 16200, training accuracy: 0.146666660905, loss: 0.935822\n",
      "epoch: 16200, testing accuracy: 0.164000004530, loss: 0.945571\n",
      "rate:  0.73 std:  45.46125796140366\n",
      "==================================================\n",
      "epoch: 16300, training accuracy: 0.143333330750, loss: 0.935553\n",
      "epoch: 16300, testing accuracy: 0.181999996305, loss: 0.945155\n",
      "rate:  0.73 std:  46.67298518436618\n",
      "==================================================\n",
      "epoch: 16400, training accuracy: 0.157333329320, loss: 0.931461\n",
      "epoch: 16400, testing accuracy: 0.156000003219, loss: 0.947768\n",
      "rate:  0.692 std:  45.82495657245583\n",
      "==================================================\n",
      "epoch: 16500, training accuracy: 0.165999993682, loss: 0.933022\n",
      "epoch: 16500, testing accuracy: 0.200000002980, loss: 0.956504\n",
      "rate:  0.71 std:  45.57719526346942\n",
      "==================================================\n",
      "epoch: 16600, training accuracy: 0.146666660905, loss: 0.932381\n",
      "epoch: 16600, testing accuracy: 0.170000001788, loss: 0.940925\n",
      "rate:  0.714 std:  46.36457035362783\n",
      "==================================================\n",
      "epoch: 16700, training accuracy: 0.147333338857, loss: 0.935044\n",
      "epoch: 16700, testing accuracy: 0.148000001907, loss: 0.951116\n",
      "rate:  0.7 std:  45.60432780200321\n",
      "==================================================\n",
      "epoch: 16800, training accuracy: 0.139333337545, loss: 0.934837\n",
      "epoch: 16800, testing accuracy: 0.175999999046, loss: 0.940599\n",
      "rate:  0.682 std:  46.14468616495764\n",
      "==================================================\n",
      "epoch: 16900, training accuracy: 0.144666671753, loss: 0.937534\n",
      "epoch: 16900, testing accuracy: 0.129999995232, loss: 0.948906\n",
      "rate:  0.708 std:  47.48683996743099\n",
      "==================================================\n",
      "epoch: 17000, training accuracy: 0.140000000596, loss: 0.935480\n",
      "epoch: 17000, testing accuracy: 0.195999994874, loss: 0.943569\n",
      "rate:  0.656 std:  44.87723193827063\n",
      "==================================================\n",
      "epoch: 17100, training accuracy: 0.146333336830, loss: 0.934334\n",
      "epoch: 17100, testing accuracy: 0.186000004411, loss: 0.944752\n",
      "rate:  0.706 std:  47.6065497506908\n",
      "==================================================\n",
      "epoch: 17200, training accuracy: 0.132666662335, loss: 0.938537\n",
      "epoch: 17200, testing accuracy: 0.164000004530, loss: 0.948436\n",
      "rate:  0.708 std:  45.72416622591616\n",
      "==================================================\n",
      "epoch: 17300, training accuracy: 0.147666662931, loss: 0.935245\n",
      "epoch: 17300, testing accuracy: 0.186000004411, loss: 0.946896\n",
      "rate:  0.69 std:  45.2928200219071\n",
      "==================================================\n",
      "epoch: 17400, training accuracy: 0.152666673064, loss: 0.932549\n",
      "epoch: 17400, testing accuracy: 0.175999999046, loss: 0.947161\n",
      "rate:  0.696 std:  46.3131832475825\n",
      "==================================================\n",
      "epoch: 17500, training accuracy: 0.152666673064, loss: 0.935594\n",
      "epoch: 17500, testing accuracy: 0.200000002980, loss: 0.943010\n",
      "rate:  0.682 std:  46.31858493983457\n",
      "==================================================\n",
      "epoch: 17600, training accuracy: 0.144333332777, loss: 0.936513\n",
      "epoch: 17600, testing accuracy: 0.197999998927, loss: 0.935629\n",
      "rate:  0.682 std:  46.114072620338916\n",
      "==================================================\n",
      "epoch: 17700, training accuracy: 0.155000001192, loss: 0.931872\n",
      "epoch: 17700, testing accuracy: 0.165999993682, loss: 0.944311\n",
      "rate:  0.74 std:  44.842756203785626\n",
      "==================================================\n",
      "epoch: 17800, training accuracy: 0.157333329320, loss: 0.935160\n",
      "epoch: 17800, testing accuracy: 0.214000001550, loss: 0.940599\n",
      "rate:  0.71 std:  46.455432936585794\n",
      "==================================================\n",
      "epoch: 17900, training accuracy: 0.147666662931, loss: 0.932553\n",
      "epoch: 17900, testing accuracy: 0.181999996305, loss: 0.943255\n",
      "rate:  0.686 std:  47.28316743061104\n",
      "==================================================\n",
      "epoch: 18000, training accuracy: 0.156333327293, loss: 0.934089\n",
      "epoch: 18000, testing accuracy: 0.162000000477, loss: 0.953330\n",
      "rate:  0.716 std:  45.58650641476533\n",
      "==================================================\n",
      "epoch: 18100, training accuracy: 0.150999993086, loss: 0.933333\n",
      "epoch: 18100, testing accuracy: 0.164000004530, loss: 0.939338\n",
      "rate:  0.704 std:  46.72180446889783\n",
      "==================================================\n",
      "epoch: 18200, training accuracy: 0.139666661620, loss: 0.937226\n",
      "epoch: 18200, testing accuracy: 0.167999997735, loss: 0.951762\n",
      "rate:  0.718 std:  45.545530033352954\n",
      "==================================================\n",
      "epoch: 18300, training accuracy: 0.149333328009, loss: 0.937668\n",
      "epoch: 18300, testing accuracy: 0.202000007033, loss: 0.944246\n",
      "rate:  0.726 std:  44.760684114052154\n",
      "==================================================\n",
      "epoch: 18400, training accuracy: 0.147333338857, loss: 0.935324\n",
      "epoch: 18400, testing accuracy: 0.175999999046, loss: 0.950054\n",
      "rate:  0.71 std:  45.6458923248024\n",
      "==================================================\n",
      "epoch: 18500, training accuracy: 0.150333330035, loss: 0.933743\n",
      "epoch: 18500, testing accuracy: 0.173999994993, loss: 0.942745\n",
      "rate:  0.696 std:  43.41247061368568\n",
      "==================================================\n",
      "epoch: 18600, training accuracy: 0.146999999881, loss: 0.935574\n",
      "epoch: 18600, testing accuracy: 0.195999994874, loss: 0.931958\n",
      "rate:  0.704 std:  46.189703953691286\n",
      "==================================================\n",
      "epoch: 18700, training accuracy: 0.153999999166, loss: 0.933599\n",
      "epoch: 18700, testing accuracy: 0.175999999046, loss: 0.943434\n",
      "rate:  0.736 std:  47.404829866383686\n",
      "==================================================\n",
      "epoch: 18800, training accuracy: 0.146666660905, loss: 0.935708\n",
      "epoch: 18800, testing accuracy: 0.159999996424, loss: 0.943670\n",
      "rate:  0.702 std:  45.598844688661636\n",
      "==================================================\n",
      "epoch: 18900, training accuracy: 0.150333330035, loss: 0.933566\n",
      "epoch: 18900, testing accuracy: 0.170000001788, loss: 0.948039\n",
      "rate:  0.702 std:  46.41963348141129\n",
      "==================================================\n",
      "epoch: 19000, training accuracy: 0.150999993086, loss: 0.932255\n",
      "epoch: 19000, testing accuracy: 0.187999993563, loss: 0.937416\n",
      "rate:  0.704 std:  47.480711725295315\n",
      "==================================================\n",
      "epoch: 19100, training accuracy: 0.144999995828, loss: 0.937443\n",
      "epoch: 19100, testing accuracy: 0.230000004172, loss: 0.930353\n",
      "rate:  0.724 std:  46.19816090906406\n",
      "==================================================\n",
      "epoch: 19200, training accuracy: 0.148666664958, loss: 0.935517\n",
      "epoch: 19200, testing accuracy: 0.181999996305, loss: 0.933959\n",
      "rate:  0.722 std:  45.875776050957064\n",
      "==================================================\n",
      "epoch: 19300, training accuracy: 0.136666670442, loss: 0.936844\n",
      "epoch: 19300, testing accuracy: 0.211999997497, loss: 0.931907\n",
      "rate:  0.714 std:  44.411360775463216\n",
      "==================================================\n",
      "epoch: 19400, training accuracy: 0.135333329439, loss: 0.937519\n",
      "epoch: 19400, testing accuracy: 0.162000000477, loss: 0.946442\n",
      "rate:  0.718 std:  47.37857508500174\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19500, training accuracy: 0.149666666985, loss: 0.932293\n",
      "epoch: 19500, testing accuracy: 0.186000004411, loss: 0.945772\n",
      "rate:  0.718 std:  45.61911146204043\n",
      "==================================================\n",
      "epoch: 19600, training accuracy: 0.134333327413, loss: 0.938659\n",
      "epoch: 19600, testing accuracy: 0.173999994993, loss: 0.941450\n",
      "rate:  0.706 std:  45.89187109785626\n",
      "==================================================\n",
      "epoch: 19700, training accuracy: 0.153999999166, loss: 0.934464\n",
      "epoch: 19700, testing accuracy: 0.159999996424, loss: 0.942138\n",
      "rate:  0.742 std:  47.538463676551764\n",
      "==================================================\n",
      "epoch: 19800, training accuracy: 0.130333334208, loss: 0.937206\n",
      "epoch: 19800, testing accuracy: 0.195999994874, loss: 0.951604\n",
      "rate:  0.69 std:  47.02507379884979\n",
      "==================================================\n",
      "epoch: 19900, training accuracy: 0.143000006676, loss: 0.933622\n",
      "epoch: 19900, testing accuracy: 0.178000003099, loss: 0.947903\n",
      "rate:  0.728 std:  48.04922726244548\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for i in range(20000):\n",
    "    batch_size = 3000\n",
    "    rand_index = np.random.choice(len(train_x), size=batch_size)\n",
    "    rand_x = train_x[rand_index]\n",
    "    rand_y = trainlabel_y[rand_index,:]\n",
    "    #print(rand_x.shape)\n",
    "    rand_x = np.reshape(rand_x, [3000, 8])\n",
    "    \n",
    "    sess.run(train_step, feed_dict={xs:rand_x, ys:rand_y, keep_prob: 0.5})\n",
    "    if i % 8 == 0:\n",
    "        lr = lr * 0.96\n",
    "    if i % 100 ==0:\n",
    "        rand_index_test = np.random.choice(len(test_x), size=500)\n",
    "        rand_x_test = test_x[rand_index_test]\n",
    "        rand_y_test = testlabel_y[rand_index_test,:]\n",
    "        rand_x_test = np.reshape(rand_x_test, [500, 8])\n",
    "        \n",
    "        train_accuracy = sess.run(accuracy, feed_dict={xs:rand_x, ys:rand_y})\n",
    "        train_loss = sess.run(cross_entropy, feed_dict={xs:rand_x, ys:rand_y})\n",
    "        print(\"epoch: {}, training accuracy: {:.12f}, loss: {:.6f}\".format(i, train_accuracy, train_loss))\n",
    "        #print (sess.run(prediction, feed_dict={xs:rand_x, ys:rand_y}))\n",
    "        \n",
    "        test_accuracy = sess.run(accuracy, feed_dict={xs:rand_x_test, ys:rand_y_test})\n",
    "        test_loss = sess.run(cross_entropy, feed_dict={xs:rand_x_test, ys:rand_y_test})\n",
    "        print(\"epoch: {}, testing accuracy: {:.12f}, loss: {:.6f}\".format(i, test_accuracy, test_loss))\n",
    "        #print(rand_x_test[0])\n",
    "        #print(rand_y_test[0])\n",
    "        #print(sess.run(prediction, feed_dict={xs:rand_x_test, ys:rand_y_test}))\n",
    "        #print(sess.run(prediction[0], feed_dict={xs:rand_x_test, ys:rand_y_test}))\n",
    "        pred = sess.run(prediction, feed_dict={xs:rand_x_test, ys:rand_y_test})\n",
    "        num1, num2 = up_and_down(pred, rand_x_test, rand_y_test)\n",
    "        print(\"rate: \",num1, \"std: \", num2)\n",
    "        print(\"==================================================\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
