{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "#加载数据\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "#训练数据\n",
    "inputs = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "#训练标签数据\n",
    "labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "#dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#把inputs更改为4维张量，第1维代表样本数量，第2维和第3维代表图像长宽， 第4维代表图像通道数, 1表示黑白\n",
    "x = tf.reshape(inputs, [-1,28,28,1])\n",
    "\n",
    "#第一层卷积\n",
    "conv1_weights = tf.Variable(tf.random_normal([3, 3, 1, 96])) #卷积核大小为3*3, 当前层深度为1， 过滤器深度为96\n",
    "#卷积\n",
    "conv1 = tf.nn.conv2d(x, conv1_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充\n",
    "conv1_biases = tf.Variable(tf.random_normal([96]))\n",
    "#激活函数Relu去线性化\n",
    "relu1 = tf.nn.relu( tf.nn.bias_add(conv1, conv1_biases) )\n",
    "#最大池化\n",
    "pool1 = tf.nn.max_pool(relu1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME') #池化层过滤器的大小为3*3, 移动步长为2，使用全0填充\n",
    "#规范化\n",
    "norm1 = tf.nn.lrn(pool1, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "norm1 = tf.nn.dropout(norm1, keep_prob)\n",
    "print(norm1.shape) \n",
    "\n",
    "#第二层卷积\n",
    "conv2_weights = tf.Variable(tf.random_normal([3, 3, 96, 256])) #卷积核大小为3*3, 当前层深度为64， 过滤器深度为128\n",
    "conv2 = tf.nn.conv2d(norm1, conv2_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充\n",
    "conv2_biases = tf.Variable(tf.random_normal([256]))\n",
    "relu2 = tf.nn.relu( tf.nn.bias_add(conv2, conv2_biases) )\n",
    "pool2 = tf.nn.max_pool(relu2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "norm2 = tf.nn.lrn(pool2, depth_radius=4, bias=1.0, alpha=0.001/9.0, beta=0.75)\n",
    "norm2 = tf.nn.dropout(norm2, keep_prob)\n",
    "print(norm2.shape) \n",
    "\n",
    "#第三层卷积\n",
    "conv3_weights = tf.Variable(tf.random_normal([3, 3, 256, 384])) #卷积核大小为3*3, 当前层深度为128， 过滤器深度为256\n",
    "conv3 = tf.nn.conv2d(norm2, conv3_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充\n",
    "conv3_biases = tf.Variable(tf.random_normal([384]))\n",
    "relu3 = tf.nn.relu( tf.nn.bias_add(conv3, conv3_biases) )\n",
    "print(relu3.shape) \n",
    "\n",
    "#第四層捲積\n",
    "conv4_weights = tf.Variable(tf.random_normal([3, 3, 384, 384])) #卷积核大小为3*3, 当前层深度为128， 过滤器深度为256\n",
    "conv4 = tf.nn.conv2d(relu3, conv4_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充\n",
    "conv4_biases = tf.Variable(tf.random_normal([384]))\n",
    "relu4 = tf.nn.relu( tf.nn.bias_add(conv4, conv4_biases) )\n",
    "print(relu4.shape) \n",
    "\n",
    "#第五層捲積\n",
    "conv5_weights = tf.Variable(tf.random_normal([3, 3, 384, 256])) #卷积核大小为3*3, 当前层深度为128， 过滤器深度为256\n",
    "conv5 = tf.nn.conv2d(relu4, conv5_weights, strides=[1, 1, 1, 1], padding='SAME') #移动步长为1, 使用全0填充\n",
    "conv5_biases = tf.Variable(tf.random_normal([256]))\n",
    "relu5 = tf.nn.relu( tf.nn.bias_add(conv5, conv5_biases) )\n",
    "pool5 = tf.nn.max_pool(relu5, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(pool5.shape) \n",
    "\n",
    "#全连接层 1\n",
    "fc1_weights = tf.Variable(tf.random_normal([4*4*256, 4096]))\n",
    "fc1_biases = tf.Variable(tf.random_normal([4096]))\n",
    "fc1 = tf.reshape(pool5, [ -1, fc1_weights.get_shape().as_list()[0] ] )\n",
    "fc1 = tf.add(tf.matmul(fc1, fc1_weights), fc1_biases)\n",
    "fc1 = tf.nn.relu(fc1)\n",
    "\n",
    "\n",
    "#全连接层 2\n",
    "fc2_weights = tf.Variable(tf.random_normal([4096, 4096]))\n",
    "fc2_biases = tf.Variable(tf.random_normal([4096]))\n",
    "fc2 = tf.reshape(fc1, [ -1, fc2_weights.get_shape().as_list()[0] ] )\n",
    "fc2 = tf.add(tf.matmul(fc2, fc2_weights), fc2_biases)\n",
    "fc2 = tf.nn.relu(fc2)\n",
    "\n",
    "\n",
    "#输出层\n",
    "out_weights = tf.Variable(tf.random_normal([4096, 10]))\n",
    "out_biases = tf.Variable(tf.random_normal([10]))\n",
    "pred = tf.add(tf.matmul(fc2, out_weights), out_biases)\n",
    "\n",
    "#定义交叉熵损失函数\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=labels))\n",
    "\n",
    "#选择优化器\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "#评估函数\n",
    "correct_prediction = tf.equal(tf.argmax(pred,1), tf.argmax(labels,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "#训练模型\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    sess.run(train_op, feed_dict={inputs: batch[0], labels: batch[1], keep_prob: 0.75})  # 训练阶段使用75%的Dropout\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = sess.run(accuracy, feed_dict={inputs:batch[0], labels: batch[1], keep_prob: 1.0}) #评估阶段不使用Dropout\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "#评估模型\n",
    "#只使用256个测试数据，机器太差， 跑10000内存溢出了\n",
    "print(\"test accuracy %g\" % sess.run(accuracy, feed_dict={inputs: mnist.test.images[:256], labels: mnist.test.labels[:256], keep_prob: 1.0})) #评估阶段不使用Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
